
sql = {
  #预加载数据
  preloadtbsql = "SELECT M.MAP_TABLE AS TABLENAME,M.RUN_SQL AS RUNSQL,M.DATA_SOURCE AS DATASOURCEID,M.REFRESH_TIME AS REFRESHTIME FROM SPARK_READY_MASTER M INNER JOIN UNIFY_JAR_READY UJR ON M.OBJ_ID = UJR.READY_ID WHERE UJR.APP_ID = '%s' AND UJR.SERVER_ID = '%s' AND UJR.JAR_ID = '%s'"
  #预加载数据详细数据
  preloadtbconfsql = "SELECT M.MAP_TABLE AS TABLENAME,D.CODE AS FIELD,D.ATTRIBUTE AS FIELDTYPE FROM SPARK_READY_MASTER M INNER JOIN SPARK_READY_DETAIL D ON M.OBJ_ID = D.READY_ID INNER JOIN UNIFY_JAR_READY UJR ON M.OBJ_ID = UJR.READY_ID WHERE UJR.APP_ID = '%s' AND UJR.SERVER_ID = '%s' AND UJR.JAR_ID = '%s' ORDER BY D.SEQ ASC"
  #数据源配置
  datasourcesql = "SELECT U.ADDRESS,U.KIND,U.TOPIC AS TABLENAME,U.CONNECT_URL AS CONNECTURL,U.USERNAME,U.PASSWORD,U.DRIVER_NAME AS DRIVER,DECOLLATOR FROM  UNIFY_DATA_SOURCE U WHERE U.OBJ_ID = '%s'"
  #kafka配置
  kafkadatasql = "SELECT U.NAME,U.KIND,U.ADDRESS AS ZKCONNECT,U.CONNECT_URL AS BROKERS,U.USERNAME,U.PASSWORD,U.TOPIC FROM  UNIFY_DATA_SOURCE U LEFT JOIN UNIFY_JAR S ON  U.OBJ_ID = S.IN_SOURCE WHERE S.APP_ID = '%s' AND S.SERVER_ID = '%s' AND S.OBJ_ID = '%s' AND U.KIND = 9"
  #全局清洗配置
  jarcleansql = "SELECT ISCLEAN,KIND AS ISSAVEALLDATA,RUN_SQL AS RUNSQL FROM UNIFY_JAR WHERE OBJ_ID = '%s'"
  #文件清洗配置
  filecleansql = "SELECT FF.COL_NUM AS COLNUM,FF.DECOLLATOR AS SEPARATOR,FF.MAP_TABLE AS TABLENAME,FF.CHAR_NUM AS BYTESPERLINE,FF.CLEAR_TIME AS RESETTIME,FF.CHARACTER AS CHARACTERENCODE FROM FLUME_FILE FF INNER JOIN UNIFY_JAR_FILE UJF ON FF.OBJ_ID = UJF.FILE_ID WHERE UJF.APP_ID = '%s' AND UJF.SERVER_ID = '%s' AND UJF.JAR_ID = '%s'"
  #列清洗配合
  colcleansql = "SELECT M.ISCLEAN, M.ISNULL,M.ENUM_VALUE AS ENUMVALUE,M.KIND,M.MAX_VALUE  AS MAXVALUE,M.MIN_VALUE  AS MINVALUE,M.LENGTH,M.CODE AS FIELD,M.START_NUM AS STARTPOS,M.END_NUM  AS ENDPOS,M.REGULAR    AS REGEX FROM FLUME_FILE_FIELD M INNER JOIN FLUME_FILE D ON M.FILE_ID = D.OBJ_ID INNER JOIN UNIFY_JAR_FILE UJF ON UJF.FILE_ID = D.OBJ_ID WHERE UJF.APP_ID = '%s' AND UJF.SERVER_ID = '%s' AND UJF.JAR_ID = '%s' ORDER BY M.SEQ ASC"
  #输出数据配置
  servicesql = "SELECT JAR_SQL AS SQL,TABLE_NAME AS MAPTABLE,SOURCE_ID AS SOURCEID,RUN_MODE AS RUNMODE,SEQ FROM UNIFY_JAR_SQL M WHERE M.APP_ID = '%s' AND M.SERVER_ID = '%s' AND M.JAR_ID = '%s' AND M.KIND = %d ORDER BY SEQ ASC"
}

default-streaming-config = {
  spark.io.compression.codec = "lzf"
  spark.speculation = true
  spark.logConf = true
  spark.serializer = "org.apache.spark.serializer.KryoSerializer"
  spark.ui.showConsoleProgress = false
}

#cleantype="java"
#cleantype="spark"
impl-class = "cn.migu.streaming.service.JavaServiceManagerImpl"

master = "spark://172.18.111.3:7078"
#master = "local"
key-class = "java.lang.String"
message-class = "java.lang.String"

key-decoder-class = "kafka.serializer.StringDecoder"
message-decoder-class = "kafka.serializer.StringDecoder"

generation-interval-sec = 60

max-rate-per-partition = 1000

max-cores = 2

executor-memory = 2048

hdfs-addr = "hdfs://hadoop"

hdfs-path="hdfs://hadoop/tmp/sjwj/"

//hdfs-addr = "hdfs://172.18.111.3:9000"

//hdfs-path="hdfs://172.18.111.3:9000/tmp/sjwj/"


jars = [${hdfs-path}hugetable-client-5.4.7-bc1.2.2-5.4.7.jar,
        ${hdfs-path}hugetable-common-5.4.7-bc1.2.2-5.4.7.jar,
        ${hdfs-path}hugetable-connector-5.4.7-bc1.2.2-5.4.7.jar,
        ${hdfs-path}hugetable-driver-5.4.7-bc1.2.2-5.4.7.jar,
        ${hdfs-path}hugetable-server-5.4.7-bc1.2.2-5.4.7.jar,
        ${hdfs-path}mysql-connector-java-5.1.7-bin.jar,
        ${hdfs-path}ojdbc14-10.2.0.4.0.jar,
        ${hdfs-path}unify-streaming-221.jar]

#${hdfs-path}metrics-core-2.2.0.jar

#t1.sh shanpao ff14af92-6b26-4e9d-afa1-56e557c91a1e 22ebbf65-f8ea-4126-b13e-bc3b4c27fb9c spark://172.18.111.3:7078 hdfs://172.18.111.3:9000 10004 sp_ugc_new.jar smm 1 1024 10 null